package com.mhh.cloud.kafka.consumer;

import kafka.api.FetchRequest;
import kafka.api.FetchRequestBuilder;
import kafka.api.PartitionOffsetRequestInfo;
import kafka.common.ErrorMapping;
import kafka.common.TopicAndPartition;
import kafka.javaapi.*;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.message.MessageAndOffset;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class TestConsumer {
	public static void main(String args[])
	{
		TestConsumer example = new TestConsumer();
		long maxReads = 100;
		String topic = "mhh_page_visits";
		int partition = 1;
		List<String> seeds = new ArrayList<String>();
		seeds.add("localhost");
		int port = Integer.parseInt("9092");
		
		try
		{
			example.run(maxReads, topic, partition, seeds, port);
		} catch(Exception e)
		{
			System.out.println("Oops:" + e);
			e.printStackTrace();			
		}
		
	}
	
	private List<String> m_replicaBrokers = new ArrayList<String>();
	
	public TestConsumer(){
		m_replicaBrokers = new ArrayList<String>();		
	}
	
	public void run(long a_maxReads, String a_topic, int a_partition, List<String> a_seedBrokers, int a_port)throws Exception
	{
		PartitionMetadata metadata = findLeader(a_seedBrokers, a_port, a_topic, a_partition);
		if(metadata == null)
		{
			System.out.println("Can't find metadata for Topic and Partition.existing");
			return;
		}
		if(metadata.leader() == null)
		{
			System.out.println("can't find leader for topic and partition.existing");
			return;			
		}
		
		String leadBroker = metadata.leader().host();
		String clientName = "Client_" + a_topic + "_" + a_partition;
		
		SimpleConsumer consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);
		long readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.EarliestTime(), clientName);
		int numErrors = 0;
		while(a_maxReads > 0)
		{
			if(consumer == null)
			{
				consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);
			}		
			
		}	
		FetchRequest req = new FetchRequestBuilder().clientId(clientName).addFetch(a_topic, a_partition, readOffset, 100000).build();
		FetchResponse fetchResponse = consumer.fetch(req);
	    if(fetchResponse.hasError())
	    {
	    	numErrors++;
	    	short code = fetchResponse.errorCode(a_topic, a_partition);
	    	System.out.println("Error fetching data from the Broker:" + leadBroker + " Reason:" + code);
	    	if(numErrors > 5)
	    		break;
	    	if(code == ErrorMapping.OffsetOutOfRangeCode())
	    	{
	    		readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName);
	    	    continue;
	    	}
	    	consumer.close();
	    	consumer = null;
	    	leadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port);
	    	continue;
	    }
	    
	    numErrors = 0;
	    long numRead = 0;
	    for(MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition))
	    {
	    	long currentOffset = messageAndOffset.offset();
	    	if(currentOffset < readOffset)
	    	{
	    		System.out.println("Found an old offset:" + currentOffset + "Expecting:" + readOffset);
	    		continue;
	    	}
	    	
	    	readOffset = messageAndOffset.nextOffset();
	    	ByteBuffer payload = messageAndOffset.message().payload();
	    	
	    	byte[] bytes = new byte[payload.limit()];
	    	payload.get(bytes);
	    	System.out.println(String.valueOf(messageAndOffset.offset()) + ":" + new String(bytes, "UTF-8"));
	    	
	    	numRead++;
	    	a_maxReads--;	    			
	    }
	    
	    if(numRead == 0)
	    {
	    	try{
	    		Thread.sleep(1000);	    		
	    	} catch(InterruptedException ie){
	    		
	    	}
	    }
	    
	    if(consumer != null)
	    	consumer.close();	
	}
	
	public static long getLastOffset(SimpleConsumer consumer, String topic, int partition, long whichTime, String clientName)
	{
		TopicAndPartition topicAndPartition = new TopicAndPartition(topic, partition);
		Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
		requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(whichTime, 1));
		kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientName);
		OffsetResponse response = consumer.getOffsetsBefore(request);
		
		if(response.hasError())
		{
			System.out.println("Error fetching data Offset Data the Broker.Reason:" + response.errorCode(topic, partition));
			return 0;
		}
		return 1;
	}

}
